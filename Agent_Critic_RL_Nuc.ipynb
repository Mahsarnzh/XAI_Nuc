{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRI7zbOFTJqm",
        "outputId": "8b723a3b-d694-48b2-b93a-3c0e00338a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Actor-Critic' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Lucasc-99/Actor-Critic.git\n",
        "!cd Actor-Critic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install torch\n",
        "! pip3 install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnSxr7z0wGrs",
        "outputId": "7a9b07f8-9f67-44c5-9bf4-c4edd70d103f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade setuptools\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ihm2c0vyFWm",
        "outputId": "081f640f-cfd8-4fd8-8e3c-a4f38e4010d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDI-uuUgzLvc",
        "outputId": "9906d3e5-ecc0-41ea-fbe7-e1014e218f67"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: brew: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install box2d-py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aigjNpyyreG",
        "outputId": "7de96305-f972-4b64-b88e-074b97a4c07d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting box2d-py\n",
            "  Using cached box2d-py-2.3.8.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5M7EaG9wx8B",
        "outputId": "1de96f49-caa1-46b0-ecaa-813a4cc90c6a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YcCaKkaxZ70",
        "outputId": "81887ed0-45fd-4382-fa51-e9a1415ecc0a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Using cached pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Collecting swig==4.* (from gym[box2d])\n",
            "  Using cached swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 -m src.cart-pole-baseline.py\n",
        "! python3 -m src.cart-pole-a2c.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uixlztHwaGR",
        "outputId": "1bf8ebe6-7153-43d1-8e3c-f9ba5dd8e806"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3: Error while finding module specification for 'src.cart-pole-baseline.py' (ModuleNotFoundError: No module named 'src'). Try using 'src.cart-pole-baseline' instead of 'src.cart-pole-baseline.py' as the module name.\n",
            "/usr/bin/python3: Error while finding module specification for 'src.cart-pole-a2c.py' (ModuleNotFoundError: No module named 'src'). Try using 'src.cart-pole-a2c' instead of 'src.cart-pole-a2c.py' as the module name.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "class A2C(nn.Module):\n",
        "\n",
        "    def __init__(self, env, hidden_size=128, gamma=.99, random_seed=None):\n",
        "        \"\"\"\n",
        "        Assumes fixed continuous observation space\n",
        "        and fixed discrete action space (for now)\n",
        "\n",
        "        :param env: target gym environment\n",
        "        :param gamma: the discount factor parameter for expected reward function :float\n",
        "        :param random_seed: random seed for experiment reproducibility :float, int, str\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if random_seed:\n",
        "            env.seed(random_seed)\n",
        "            torch.manual_seed(random_seed)\n",
        "\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.hidden_size = hidden_size\n",
        "        self.in_size = len(env.observation_space.sample().flatten())\n",
        "        self.out_size = self.env.action_space.n\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(self.in_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, self.out_size)\n",
        "        ).double()\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(self.in_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        ).double()\n",
        "\n",
        "    def train_env_episode(self, render=False):\n",
        "        \"\"\"\n",
        "        Runs one episode and collects critic values, expected return,\n",
        "        :return: A tensor with total/expected reward, critic eval, and action information\n",
        "        \"\"\"\n",
        "        rewards = []\n",
        "        critic_vals = []\n",
        "        action_lp_vals = []\n",
        "\n",
        "        # Run episode and save information\n",
        "\n",
        "        observation = self.env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if render:\n",
        "                self.env.render()\n",
        "\n",
        "            observation = torch.from_numpy(observation).double()\n",
        "\n",
        "            # Get action from actor\n",
        "            action_logits = self.actor(observation)\n",
        "\n",
        "            action = Categorical(logits=action_logits).sample()\n",
        "\n",
        "            # Get action probability\n",
        "            action_log_prob = action_logits[action]\n",
        "\n",
        "            # Get value from critic\n",
        "            pred = torch.squeeze(self.critic(observation).view(-1))\n",
        "\n",
        "            # Write prediction and action/probabilities to arrays\n",
        "            action_lp_vals.append(action_log_prob)\n",
        "            critic_vals.append(pred)\n",
        "\n",
        "            # Send action to environment and get rewards, next state\n",
        "\n",
        "            observation, reward, done, info = self.env.step(action.item())\n",
        "            rewards.append(torch.tensor(reward).double())\n",
        "\n",
        "        total_reward = sum(rewards)\n",
        "\n",
        "        # Convert reward array to expected return and standardize\n",
        "        for t_i in range(len(rewards)):\n",
        "\n",
        "            for t in range(t_i + 1, len(rewards)):\n",
        "                rewards[t_i] += rewards[t] * (self.gamma ** (t_i - t))\n",
        "\n",
        "        # Convert output arrays to tensors using torch.stack\n",
        "        def f(inp):\n",
        "            return torch.stack(tuple(inp), 0)\n",
        "\n",
        "        # Standardize rewards\n",
        "        rewards = f(rewards)\n",
        "        rewards = (rewards - torch.mean(rewards)) / (torch.std(rewards) + .000000000001)\n",
        "\n",
        "        return rewards, f(critic_vals), f(action_lp_vals), total_reward\n",
        "\n",
        "    def test_env_episode(self, render=True):\n",
        "        \"\"\"\n",
        "        Run an episode of the environment in test mode\n",
        "        :param render: Toggle rendering of environment :bool\n",
        "        :return: Total reward :int\n",
        "        \"\"\"\n",
        "        observation = self.env.reset()\n",
        "        rewards = []\n",
        "        done = False\n",
        "        while not done:\n",
        "\n",
        "            if render:\n",
        "                self.env.render()\n",
        "\n",
        "            observation = torch.from_numpy(observation).double()\n",
        "\n",
        "            # Get action from actor\n",
        "            action_logits = self.actor(observation)\n",
        "            action = Categorical(logits=action_logits).sample()\n",
        "\n",
        "            observation, reward, done, info = self.env.step(action.item())\n",
        "            rewards.append(reward)\n",
        "\n",
        "        return sum(rewards)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_loss(action_p_vals, G, V, critic_loss=nn.SmoothL1Loss()):\n",
        "        \"\"\"\n",
        "        Actor Advantage Loss, where advantage = G - V\n",
        "        Critic Loss, using mean squared error\n",
        "        :param critic_loss: loss function for critic   :Pytorch loss module\n",
        "        :param action_p_vals: Action Log Probabilities  :Tensor\n",
        "        :param G: Actual Expected Returns   :Tensor\n",
        "        :param V: Predicted Expected Returns    :Tensor\n",
        "        :return: Actor loss tensor, Critic loss tensor  :Tensor\n",
        "        \"\"\"\n",
        "        assert len(action_p_vals) == len(G) == len(V)\n",
        "        advantage = G - V.detach()\n",
        "        return -(torch.sum(action_p_vals * advantage)), critic_loss(G, V)\n"
      ],
      "metadata": {
        "id": "bXsgTASjw71j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cart-pole reinforcement learning environment:\n",
        "Agent learns to balance a pole on a cart\n",
        "\n",
        "a2c: Agent uses Advantage Actor Critic algorithm\n",
        "\n",
        "\"\"\"\n",
        "import gym\n",
        "# from src.a2c import A2C\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "LR = .01  # Learning rate\n",
        "SEED = None  # Random seed for reproducibility\n",
        "MAX_EPISODES = 1000  # Max number of episodes\n",
        "\n",
        "# Init actor-critic agent\n",
        "agent = A2C(gym.make('CartPole-v0'), random_seed=SEED)\n",
        "\n",
        "# Init optimizers\n",
        "actor_optim = optim.Adam(agent.actor.parameters(), lr=LR)\n",
        "critic_optim = optim.Adam(agent.critic.parameters(), lr=LR)\n",
        "\n",
        "#\n",
        "# Train\n",
        "#\n",
        "\n",
        "r = []  # Array containing total rewards\n",
        "avg_r = 0  # Value storing average reward over last 100 episodes\n",
        "\n",
        "for i in range(MAX_EPISODES):\n",
        "    critic_optim.zero_grad()\n",
        "    actor_optim.zero_grad()\n",
        "\n",
        "    rewards, critic_vals, action_lp_vals, total_reward = agent.train_env_episode(render=False)\n",
        "    r.append(total_reward)\n",
        "\n",
        "    l_actor, l_critic = agent.compute_loss(action_p_vals=action_lp_vals, G=rewards, V=critic_vals)\n",
        "\n",
        "    l_actor.backward()\n",
        "    l_critic.backward()\n",
        "\n",
        "    actor_optim.step()\n",
        "    critic_optim.step()\n",
        "\n",
        "    # Check average reward every 100 episodes, print, and end script if solved\n",
        "    if len(r) >= 100:  # check average every 100 episodes\n",
        "\n",
        "        episode_count = i - (i % 100)\n",
        "        prev_episodes = r[len(r) - 100:]\n",
        "        avg_r = sum(prev_episodes) / len(prev_episodes)\n",
        "        if len(r) % 100 == 0:\n",
        "            print(f'Average reward during episodes {episode_count}-{episode_count+100} is {avg_r.item()}')\n",
        "        if avg_r > 195:\n",
        "            print(f\"Solved CartPole-v0 with average reward {avg_r.item()}\")\n",
        "            break\n",
        "\n",
        "\n",
        "#\n",
        "# Test\n",
        "#\n",
        "# for _ in range(100):\n",
        "#     agent.test_env_episode(render=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A1aHCVhwtAs",
        "outputId": "67c1cc09-5e42-43c5-b0a0-f5356dcd36d4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward during episodes 0-100 is 32.83\n",
            "Average reward during episodes 100-200 is 85.25\n",
            "Average reward during episodes 200-300 is 164.21\n",
            "Average reward during episodes 300-400 is 153.41\n",
            "Average reward during episodes 400-500 is 186.53\n",
            "Average reward during episodes 500-600 is 153.64\n",
            "Average reward during episodes 600-700 is 181.51\n",
            "Average reward during episodes 700-800 is 177.78\n",
            "Solved CartPole-v0 with average reward 195.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mf8LfrT4ww2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vsov5kh_wwzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cart-pole reinforcement learning environment:\n",
        "Agent learns to balance a pole on a cart\n",
        "\n",
        "Baseline: Agent selects moves at random\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import gym\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "\n",
        "t_steps = []\n",
        "for i_episode in range(1000):\n",
        "\n",
        "    observation = env.reset()  # Get initial observation\n",
        "\n",
        "    for t in range(100):\n",
        "\n",
        "        # env.render()\n",
        "\n",
        "        action = env.action_space.sample()  # Get a random action (left or right)\n",
        "\n",
        "        observation, reward, done, info = env.step(action)  # Get next step of the game\n",
        "\n",
        "        if done:\n",
        "            t_steps.append(t + 1)\n",
        "            break\n",
        "    break\n",
        "for t in t_steps:\n",
        "    print(t)\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_054obbBwwsv",
        "outputId": "72a5de98-f1f5-4233-f2e5-f8ff251c9ca7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "18P7Mmg3wx-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AuMYJtc-w76m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EtY56XfGw731"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7EyQqzTa0oec"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}